CI 5330 Section 002: Learning Analytics in the Knowledge Age
============================================================

## Instructor Information

* __Bodong Chen, Assistant Professor__
* Email: chenbd@umn.edu
* Phone: (773) 850-1032
* Office: 1954 Buford Avenue, 210B Learning & Environmental Sciences Building, St. Paul, MN 55108
* Office Hours: By Appointment

## Course Description

### Overview

Learning analytics is an nascent field of research that aspires to "turn educational data into actionable knowledge, in order to optimize learning and the environments in which it occurs." This course aims to provide a *general, non-technical* survey of learning analytics and its application in various educational contexts. In particular, we will discuss theoretical foundations of this field, explore new forms of assessment, get acquainted with popular data mining techniques, review learning analytical tools deployed in various settings, and design/develop new analytic tools by ourselves, with emphasis on assessing emergent competencies of interest in the knowledge age. Additional supports will be provided as well for students interested in pursuing specific issues in any of these areas. Overall, this will be a great course to get a broad overview of the status quo and future directions of learning analytics.

### Audience

The course is designed for a broad audience. All graduate students interested in learning analytics and its application in specific educational areas (e.g., STEM, literacies, life-long education) are welcomed.

Prerequisites: None, but some prior knowledge in learning theories, assessment, and/or data science recommended.

### Objectives

By the end of the course, students should:

1. Understand the logic of analytics;
2. Identify and describe key epistemological, pedagogical, ethical, and technical factors related to the design of learning analytics;
3. Be familiar with the basics of finding, cleaning, and using educational data;
4. Understand some of these popular data mining techniques, including predictive models, text analysis, relationship mining, and social networks;
5. Develop beginning skills necessary to plan and design learning analytics;
6. Be able to apply learned data analytic skills in their own research.

## Course Design

This is a _Knowledge Building_ course, which means all course participants (including the instructor) are collectively producing ideas and knowledge as a community, to solve authentic learning analytics problems \footnote{Scardamalia, M. and Bereiter, C. (2003). Knowledge building. In Guthrie, J. W., editor, Encyclopedia of education, volume 17, pages 1370–1373. Macmillan Reference, New York, NY, 2 edition.}. Our top-level goal in this course will be to work as a knowledge building team, _living_ and _exploring_ the capacity of learning analytics in supporting growth in learning in different domains. This overarching goal will be interwoven throughout this course. We will advance this goal through analysis of readings, case studies, and innovative design.

### Course Timeline

_The first five weeks_ are designed to provide an introduction to the field of learning analytics, including its roots, basic logic, data mining techniques, and case studies. These weeks feature both _theoretical discussion_, with emphasis on the assumptions underlying analytics tools and projects, and _hands-on learning activities_. During the process, we will continue to articulate our collective design goal and form _working groups (WGs)_ around emergent sub-goals in different design contexts.

_The second part_ of the course features five "themes" representing key research areas in the field of learning analytics. Each student will sign up for one theme, and students under a same theme form a special interest groups (SIG). Each SIG is expected to take a lead on its theme---presenting key ideas, leading discussion, and making connections with our collective goal. During the time, each WG will keep advancing their designs, in the forms of design specs, mock-ups, or functioning prototypes.

The class will use _the final weeks_ to further advance our designs and create synthesis. Each WG will present their work in front of the class. We will together reflect on our designs and the extent to which our collective goal is achieved.

### Group Collaboration

As mentioned above, students will work in small groups throughout the course.

- SIGs: Take a lead of one class meeting focusing on a chosen theme. Design with the instructor.
- WGs: Design analytics for a chosen context. Examples of design contexts includ the following:
- Knowledge Building Analytics: Knowledge Building (KB) as a distinctive pedagogy has a long-standing interests in analytics. While using available analytic tools designed to support KB, students are encouraged to advance KB analytics to assess sophisticated phenomena, such as the status of dialogues, meta-dialogues, and community cohesion.
- [Institute for the Scholarship of Assessment, Learning, and Teaching (iSALT)](http://www.mnsu.edu/its/academic/isalt.html), Minnesota State University, Mankato: iSALT is looking for ways to improve scholarly activities related to effective teaching, learning, and assessment practices, especially through learning analytics. We would have guests from iSALT to present their real-world challenges. Interested students could rally to tackle their challenges.
- UMN (George)
- Students are more than welcomed to bring in their own design contexts.

### Supporting Environments

- Online Learning Environment: [Knowledge Forum (KF)](http://kf.utoronto.ca:8080/kforum/)
- Register for an account
- Signup for the course using access code: "last"
- I encourage students to communicate thoughts using media of their choices - e.g., a blog, Twitter, Youtube - using a #LAUMN course hashtag

<!-- ## Analytics Tools

- R
- Gephi
- RapidMiner
- ...
-->

## Course Evaluation

#### Parameters

- _Group- and Individual-Assessment_: Students will be assessed both individually and as a group (SIG and WG)
- _Self- and Peer-Assessment_: Students will be assessing both by themselves and peers, based their contributions to the community and personal growth

#### Grading

- Class participation, 30%
- SIG theme presentation, 25%
- WG final presentation, 25%
- Final assignment, 20%

Class participation involves active (and constructive) participation in online and offline discussions. Evaluation will be based on both numeric metrics exported from Knowledge Forum and qualitative assessment of one's contribution to discussions.

Group presentations will be peer-assessed: When one group presents, other groups will evaluate the presentation following a given rubric. Students in a same group get a same score. Aggregated results will be provided to them after the class.

Final assignment: Students would have the choice among _writing a reflective essay (not exceeding 4,000 words including references)_, _preparing a portfolio Knowledge Forum note_ reflecting on one's journey in the course, _producing design documents of a proposed analytic tool_, or _developing a functioning prototype of learning analytics_.

\newpage

## __Class Schedule__

- Special Topics: Learning Analytics
- Professor Bodong Chen

### Overview

Week/Date | Topic | Important Notes
--- | --- | ---
Week 1, Jan 22 | Introduction | In-take Survey
Week 2, Jan 29 | Learning Analytics: A Brief Overview |
Week 3, Feb 5  | What to Assess? "New Competencies" |
Week 4, Feb 12 | Explore Hidden Assumptions | Class starts at 5pm; meet with Knight
Week 5, Feb 19 | Educational Data Mining | SIG signup
Week 6, Feb 26 | Cases and Examples of Learning Analytics | WG signup; meet with Karypis
Week 7, Mar 5  | Data Wrangling | Meet Stian Haklev
Week 8, Mar 12 | Learning and Knowledge Growth (theme 1) |
Week 9, Mar 19 | Spring Break; LAK Conference; NO CLASS |
Week 10, Mar 26 | Social Networks (theme 2) |
Week 11, Apr 2  | Mining of Text and Discourse (theme 3) |
Week 12, Apr 9  | Temporality in Learning (theme 4) |
Week 13, Apr 16 | AERA Conference; Group/Individual Study |
Week 14, Apr 23 | Prediction and Intervention (theme 5) |
Week 15, Apr 30 | Final Presentation |
Week 16, May 7  | Final Presentation |




#### __Week 1: Introduction__

Readings: None

Learning Activities

- Complete [in-take survey](link)
- Get familiar with Knowledge Forum (KF)
- KF Discussion
  1. Introduce yourself and tell people why you're here!
  2. Discuss learning analytics research and projects you are aware of

#### __Week 2: Learning Analytics: A Brief Overview__

Readings

- Siemens, G. (2013). [Learning Analytics: The Emergence of a Discipline](http://abs.sagepub.com/content/57/10/1380). American Behavioral Scientist, 0002764213498851. doi:10.1177/0002764213498851
- Shum, S. B. (2012). [UNESCO Policy Brief: Learning Analytics](http://www.iite.unesco.org/publications/3214711/). Technical Report, UNESCO Institute for Information Technologies in Education.

<!-- - Optional
- Long, P. and Siemens, G. (2011). [Penetrating the Fog: Analytics in Learning and Education](http://www.educause.edu/EDUCAUSE+Review/EDUCAUSEReviewMagazineVolume46/PenetratingtheFogAnalyticsinLe/235017). Educause Review, 46(5):30–32.
- Siemens, G. (2012). Learning analytics. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK ’12 (p. 4). New York, New York, USA: ACM Press. doi:10.1145/2330601.2330605
- [Video - Introduction to LAK13, by George Siemens](https://s3.amazonaws.com/LAK13/player.html)
- Suthers, D., & Rosen, D. (2011). A unified framework for multi-level analysis of distributed learning. Proceedings of the 1st International Conference on Learning Analytics and Knowledge - LAK ’11 (pp. 64–74). New York, New York, USA: ACM Press. doi:10.1145/2090116.2090124
- Olmos, M. and Corrin, L. (2012). Academic analytics in a medical curriculum: Enabling educational excellence. Australasian Journal of Educational Technology, 28(1):1–15.
- Johnson, J. A. (2014). The ethics of big data in higher education. International Review of Information Ethics, 7:3–10.
- [Video - Carolyn Rosé talking about LA and EDM for learning discourses](http://isls-naples.psy.lmu.de/intro/all-webinars/rose_all/index.html)
-->

#### Activities

- KF Discussion: Discuss readings in KF
- Start planning the final analytics project

#### __Week 3: What to Assess: "New Competencies" in the Knowledge Age__

#### Readings

<!--
- Soland, J., Hamilton, L. S., and Stecher, B. M. (2013). [Measuring 21st century competencies: Guidance for educators](http://asiasociety.org/files/gcen-measuring21cskills.pdf). Report, Asia Society Global Cities Education network.
-->

- Binkley, M., Erstad, O., Herman, J., Raizen, S., Riple, M., Miller-Ricci, M., and Rumble, M. (2012). Defining 21st century skills. In Griffin, P., McGaw, B., and Care, E., editors, Assessment and Teaching of 21st Century Skills, chapter 2, pages 17–66. Springer.
- Shum, S. B. and Crick, R. D. (2012). [Learning dispositions and transferable competencies](http://oro.open.ac.uk/32823/). In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK ’12, page 92, New York, New York, USA. ACM Press.
- Dawson, S. and Siemens, G. (2014). [Analytics to literacies: The development of a learning analytics framework for multiliteracies assessment](http://www.irrodl.org/index.php/irrodl/article/view/1878/3006). International Review of Research in Open and Distance Learning, 15(4):284–305.


#### Activities

- KF Discussion

#### __Week 4: Explore Hidden Assumptions: Epistemology, Pedagogy, Assessment and Learning Analytics__

#### Readings

- Knight, S., Buckingham Shum, S., and Littleton, K. (2014). [Epistemology, assessment, pedagogy: where learning meets analytics in the middle space](http://epress.lib.uts.edu.au/journals/index.php/JLA/article/view/3538). Journal of Learning Analytics, 1(2):23–47.
- Stahl, G. (2013). Learning across Levels. International Journal of Computer-Supported Collaborative Learning, 8(1):1–12.

#### Activities

- Virture meeting with our guest speaker, Simon Knight, Open University
- KF discussion


#### __Week 5: Educational Data Mining: A Survey of Important Algorithms, Techniques, and Their Applications__

#### Readings

- Scheuer, O. and McLaren, B. M. (2012). [Educational data mining](http://www.academia.edu/download/30948439/ScheuerMcLaren-EducationalDataMining-EncyOfLearningScience2011.pdf). In Ency- clopedia of the Sciences of Learning, pages 1075–1079. Springer.
- Baker, R.S.J.d., Yacef, K. (2009) The State of Educational Data Mining in 2009: A Review and Future Visions. Journal of Educational Data Mining, 1 (1), 3-17.
- Bienkowski, M., Feng, M., & Means, B. (2012). [Enhancing teaching and learning through educational data mining and learning analytics](http://tech.ed.gov/wp-content/uploads/2014/03/edm-la-brief.pdf). Washington, DC: U.S. Department of Education.
- Optional
- Romero, C., Ventura, S. (2007) A Survey from 1995 to 2005. Expert Systems with Applications, 33 (1), 135-146.
- Hey, A. J. G., Tansley, S., and Tolle, K. M. (2009). [The fourth paradigm: data-intensive scientific discovery](http://research.microsoft.com/en-us/collaboration/fourthparadigm/). Microsoft Research Redmond, WA.
- Resources from [Ryan Baker's "MOOT"](http://www.columbia.edu/~rsb2162/bigdataeducation.html)

#### Activities

- __SIG signup__; 2-3 students per group
- KF discussion
- Discuss readings
- Further articulate design goals

#### __Week 6: Cases and Examples of Learning Analytics__

#### Readings and Resources:

- Arnold, K.E. (2010). Signals: Applying academic analytics. Educause Quarterly, 33, 1-10.
- [Social Networks Adapting Pedagogical Practice (SNAPP)](http://www.snappvis.org/)
- [Improving Retention by Identifying and Supporting "At-Risk" Students](http://www.educause.edu/ero/article/improving-retention-identifying-and-supporting-risk-students)
- Teplovs, C., Donoahue, Z., Scardamalia, M., and Philip, D. (2007). Tools for Concurrent, Embedded, and Transformative Assessment of Knowledge Building Processes and Progress. In Proceedings of the 8th iternational conference on Computer supported collaborative learning, pages 721– 723, New Brunswick, New Jersey, USA. International Society of the Learning Sciences.

#### Activities

- Meet our guest speaker from iSALT, Vicky Cai, who would present their design context
- Discuss readings in KF
- Explore analytic tools provided in Knowledge Forum
- Finalize design goals; __WG signup__

#### __Week 7: Working with Data__

- Tony Hirst's talk
- Meet with Stian Haklev

#### __Week 7: Learning and Knowledge Growth (theme 1)__

#### Readings

- Schwarz, C. V., Reiser, B. J., Davis, E. A., Kenyon, L., Ach ́er, A., Fortus, D., Shwartz, Y., Hug, B., and Krajcik, J. (2009). Developing a learning progression for scientific modeling: Making scientific modeling accessible and meaningful for learners. Journal of Research in Science Teaching, 46(6):632–654.
- Bull, S. and Kay, J. (2010). Open learner models. In Nkambou, R., Bordeau, J., and Miziguchi, R., editors, Advances in Intelligent Tutoring Systems, chapter 15, pages 318–338. Springer.
- Desmarais, M. C., & Baker, R. S. J. d. (2011). A review of recent advances in learner and skill modeling in intelligent learning environments. User Modeling and User-Adapted Interaction, 22(1-2), 9–38. doi:10.1007/s11257-011-9106-8

#### Activities

- Designed by the SIG (with assistance from the instructor)

#### __Week 8: Social Networks (theme 2)__

#### Readings

- Haythornthwaite, C. (1996). Social network analysis: An approach and technique for the study of information exchange. Library & Information Science Research, 18(4):323–342.
- Grunspan, D. Z., Wiggins, B. L., & Goodreau, S. M. (2014). Understanding Classrooms through Social Network Analysis: A Primer for Social Network Analysis in Education Research. CBE-Life Sciences Education, 13(2), 167–178. doi:10.1187/cbe.13-08-0162
- Oshima, J., Oshima, R., and Matsuzawa, Y. (2012). Knowledge Building Discourse Explorer: a social network analysis application for knowledge building discourse. Educational Technology Research and Development, 60(5):903–921.
- Chen, B., Chen, X, & Xing, W. (submitted). Twitter Archeology of Learning Analytics and Knowledge Conferences. Paper submitted to the 2015 Learning Analytics and Knowledge Conference.

#### Activities

- Designed by the SIG (with assistance from the instructor)
- Gephi

#### __Week 9: Mining of Text and Discourse (theme 3)__

#### Readings

- Rohrer, R., Ebert, D., and Sibert, J. (1998). The shape of Shakespeare: visualizing text using implicit surfaces. In Proceedings of IEEE Symposium on Information Visualization, pages 121–129. IEEE Comput. Soc.
- Rose, C. P., Wang, Y.-C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., and Fis- cher, F. (2008). Analyzing collaborative learning processes automatically: Exploiting the advances of com- putational linguistics in computer-supported collaborative learning. International Journal of Computer- Supported Collaborative Learning, 3(3):237–271.
- Optional
- Shermis, M. D. (2014). State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration. Assessing Writing, 20, 53–76.
- Rose, C. P. and Tovares, A. (2014). What Sociolinguistics and Machine Learning Have to Say to One Another about Interaction Analysis. In Resnick, L., Asterhan C., and Clarke S., editors, Socializing Intelligence Through Academic Talk and Dialogue. American Educational Research Association, Washington, D.C.
- Simsek, D., Buckingham Shum, S., Sandor, A., De Liddo, A., and Ferguson, R. (2013). Xip dashboard: visual analytics from automated rhetorical parsing of scientific metadiscourse. In 1st International Workshop on Discourse-Centric Learning Analytics.

#### Activities

- Designed by the SIG (with assistance from the instructor)
- ManyEyes; LightSIDE

#### __Week 10: Temporality in Learning (theme 4)__

#### Readings

<!--
- Zhang, J., Lee, J., and Wilde, J. (2012). Metadiscourse to foster student collective responsibility for deepening inquiry. In van Aalst, J., Thompson, K., Jacobson, M. J., and Reimann, P., editors, The future of learning: Proceedings of the 10th international conference of the learning sciences (ICLS 2012) - Volume 1, Full Papers, pages 395–402. ISLS, Sydney, Australia.
- Resendes, M., Chen, B., Acosta, A., and Scardamalia, M. (2013). The Effect of Formative Feedback on Vocabulary Use and Distribution of Vocabulary Knowledge in a Grade Two Knowl- edge Building Class. In Rummel, N., Kapur, M., Nathan, M., and Puntambekar, S., editors, To See the World and a Grain of Sand: Learning across Levels of Space, Time, and Scale: CSCL 2013 Conference Proceedings Volume 1 - Full Papers & Symposia, pages 391–398. International Society of the Learning Sciences.
- Graesser, A. C. and McNamara, D. S. (2011). Computational analyses of multilevel discourse comprehension. Topics in Cognitive Science, 3(2):371–398.
-->

- Chen, B. and Resendes, M. (2014). Uncovering what matters: Analyzing transitional relations among contribution types in knowledge-building discourse. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge - LAK ’14, pages 226–230, New York, New York, USA. ACM Press.



<!-- ### Personalization and Adaptation (theme 4) -->

#### __Week 11: Prediction and Intervention (theme 5)__

#### Readings

- Pardos, Z.A., Baker, R.S.J.d., San Pedro, M.O.C.Z., Gowda, S.M., Gowda, S.M. (2013). Affective states and state tests: Investigating how affect throughout the school year predicts end of year learning outcomes. In Proceedings of the 3rd International Conference on Learning Analytics and Knowledge.
<!-- - DeBoer, J. and Breslow, L. (2014). Tracking progress: Predictors of students’ weekly achievement during a circuits and electronics mooc. In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14, pages 169–170, New York, NY, USA. ACM.
- Kloft, M., Stiehler, F., Zheng, Z., and Pinkwart, N. (2014). Predicting mooc dropout over weeks using machine learning methods. In Modeling Large Scale Social Interaction in Massively Open Online Courses Workshop (EMNLP 2014).-->
- Baker, R. S. J. d., D’Mello, S. K., Rodrigo, M. M. T., & Graesser, A. C. (2010). Better to be frustrated than bored: The incidence, persistence, and impact of learners’ cognitive–affective states during interactions with three different computer-based learning environments. International Journal of Human-Computer Studies, 68(4), 223–241. doi:10.1016/j.ijhcs.2009.12.003

Activities
- RapidMiner

<!-- ### Classroom Interventions with Learning Analytics -->

#### __Week 12: Preparing for Final Presentations__

WGs work to prepare for the final presentations.

#### __Week 13 and 14: Final Presentations and Reflection__

WGs present their design or implementation of learning analytics to address an authentic problem.

#### Reflection

- Ethics and privacy issues
- Classroom interventions with learning analytics






\newpage

## Additional Bibliography

- Baker, R. S. J. d., Gowda, S. M., and Corbett, A. (2011). Automatically detecting a student’s preparation for future learning: Help use is key. In Proceedings of the 4th International Conference on Educational Data Mining, pages 179–188.
- Baker, R. S., Hershkovitz, A., Rossi, L. M., Goldstein, A. B., and Gowda, S. M. (2013). Predicting Robust Learning With the Visual Form of the Moment-by-Moment Learning Curve. Journal of the Learning Sciences, 22(4):639–666.
- Bienkowski, M., Feng, M., and Means, B. (2012). Enhancing Teaching and Learning Through Educational Data Mining and Learning Analytics: An issue brief.
- Blikstein, P. (2011). Using learning analytics to assess students’ behavior in open-ended programming tasks. In Proceedings of the 1st International Conference on Learning Analytics and Knowl- edge - LAK ’11, page 110, New York, New York, USA. ACM Press.
- Burstein, J., Marcu, D., Andreyev, S., and Chodorow, M. (2001). Towards automatic classification of discourse elements in essays. In Proceedings of the 39th annual Meeting on Association for Computational Linguistics, pages 98–105. Association for Computational Linguistics.
- Calvo, R. A. and D’Mello, S. (2010). Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affective Computing, 1(1):18– 37.
- Chiu, M. M. (2008). Flowing Toward Correct Contributions During Group Problem Solving: A Statistical Discourse Analysis. Journal of the Learning Sciences, 17(3):415–463.
- Coffrin, C., Corrin, L., de Barba, P., and Kennedy, G. (2014). Visualizing patterns of student engagement and performance in MOOCs. In Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK ’14, pages 83–92, New York, New York, USA. ACM Press.
- D’Mello, S., Olney, A., and Person, N. (2010). Mining Collaborative Patterns in Tutorial Dialogues. Journal of Educational Data Mining, 2(1):1–37.
- Dyke, G., Kumar, R., Ai, H., and Ros ́e, C. P. (2012). Challenging assumptions: Using sliding window visualizations to reveal time-based irregularities in CSCL processes. In van Aalst, J., Thompson, K., Jacobson, M. J., and Reimann, P., editors, The future of learning: Proceedings of the 10th international conference of the learning sciences (ICLS 2012) - Volume 1, Full Papers, pages 363–370. ISLS, Sydney, Australia.
- Ferguson, R. (2012) Learning analytics: drivers, developments and challenges. International Journal of Technology Enhanced Learning (IJTEL), 4 (5/6), 304-317.
- Gobert, J. D., Sao Pedro, M., Raziuddin, J., and Baker, R. S. (2013). From Log Files to Assessment Metrics: Measuring Students’ Science Inquiry Skills Using Educational Data Mining. Journal of the Learning Sciences, 22(4):521–563.
- Halatchliyski, I., Hecking, T., G ̈ohnert, T., and Hoppe, H. U. (2013). Analyzing the flow of ideas and profiles of contributors in an open learning community. In Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK ’13, page 66, New York, USA. ACM Press.
- Halevy, A., Norvig, P., and Pereira, F. (2009). The unreasonable effectiveness of data. Intelligent Systems, IEEE, 24(2):8–12.
- Haythornthwaite, C., de Laat, M., and Dawson, S. (2013). Introduction to the Special Issue on Learning Analytics. American Behavioral Scientist, 57(10):1371–1379.
- Holloway, T., Bozicevic, M., and B ̈orner, K. (2007). Analyzing and visualizing the semantic coverage of Wikipedia and its authors. Complexity, 12(3):30–40.
- Howley, I., Mayfield, E., and Ros ́e, C. P. (2013). Linguistic analysis methods for studying small groups.
- Lee, V. R., & Drake, J. (2013). Quantified Recess: Design of an Activity for Elementary Students Involving Analyses of Their Own Movement Data. In Proceedings of the 12th International Conference on Interaction Design and Children (pp. 273–276). New York, NY, USA: ACM. doi:10.1145/2485760.2485822
- Martin, T. and Sherin, B. (2013). Learning Analytics and Computational Tech- niques for Detecting and Evaluating Patterns in Learning: An Introduction to the Special Issue. Journal of the Learning Sciences, 22(4):511–520.
- Mattingly, K., Rice, M., and Berge, Z. (2012). Learning analytics as a tool for closing the assessment loop in higher education. Knowledge Management & E-Learning: An International Journal, 4(3):236–247.
- New York Times. (2014, August 6). [Is Big Data Spreading Inequality?](http://www.nytimes.com/roomfordebate/2014/08/06/is-big-data-spreading-inequality) Retrieved August 20, 2014
- Romero, C., Ventura, S., Espejo, P. G., and Herv ́as, C. (2008). Data Mining Algo- rithms to Classify Students. Network, pages 20–21.
- Siemens, G. (2013) Learning Analytics: The Emergence of a Discipline. American Behavioral Scientist, 57 (10), 1380-1400.
- Siemens, G., Baker, R.S.J.d. (2012) Learning Analytics and Educational Data Mining: Towards Communication and Collaboration. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge.
- Suthers, D. and Rosen, D. (2011). A unified framework for multi-level analysis of distributed learning. In Proceedings of the 1st International Conference on Learning Analytics and Knowledge - LAK ’11, pages 64–74, New York, New York, USA. ACM Press.
- Wise, A. F. (2014). Designing pedagogical interventions to support student use of learning analytics. In Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK ’14, pages 203–211, New York, New York, USA. ACM Press.
- [Your cognitive future
How next-gen computing changes the way we live and work, Part 1](http://public.dhe.ibm.com/common/ssi/ecm/gb/en/gbe03641usen/GBE03641USEN.PDF)

/newpage

## Additional Course Policies

### Receipt of Final Grade
University policies do not permit the posting of final course grades nor the reporting of these grades over the telephone. If you would like a record of your course grade before it is available via the University website, contact your instructor for arrangements.

The grading for this course is as follows:

A 	95 – 100%			D+	67 – 69%
A- 	90 – 94%			D	60 – 66%
B+ 	87 – 89%			F	Below 60%
B 	84 – 86%
B- 	80 – 83%
C+ 	77 – 79%
C 	74 – 76%
C- 	70 – 73%

### Email Correspondence
Please feel free to email me when you have course questions at chenbd@umn.edu. I am happy to help you. You can expect a response to your emails within a reasonable time frame of 24 - 48 hours. Responses likely will not be immediate, so do not wait until the last minute to contact me. I will generally not be responding to emails over the weekend, nor will I expect you to (though I do check and reply at times it should not be expected). I will use your UMN student email account for all email correspondence. I will not use personal email addresses (e.g. personal gmail, yahoo, or hotmail accounts). Please be sure to check you UMN student email at least a couple times a week. Consider forwarding emails to a personal account that you check more often to ensure timely receipt of communications.




### Relevant University Policies

#### UNIVERSITY GRADING SYSTEM

Grading and Transcripts
The University utilizes plus and minus grading on a 4.000 cumulative grade point scale in accordance with the following:
A	4.000 - Represents achievement that is outstanding relative to the level necessary to meet course requirements
A-	3.667
B+	3.333
B	3.000 - Represents achievement that is significantly above the level necessary to meet course requirements
B-	2.667
C+	2.333
C	2.000 - Represents achievement that meets the course requirements in every respect
C-	1.667
D+	1.333
D	1.000 - Represents achievement that is worthy of credit even though it fails to meet fully the course requirements
S	Represents achievement that is satisfactory, which is equivalent to a C- or better.

This means that the grade that you have earned in this course based on the percentage scale above will then be documented on your transcript according to this 4.000 scale and letter grade, not as a percentage.

For additional information about grades, please refer to: http://policy.umn.edu/Policies/Education/Education/GRADINGTRANSCRIPTS.html.


Definition of Grades
A - achievement that is outstanding relative to the level necessary to meet course requirements.
B - achievement that is significantly above the level necessary to meet course requirements.
C - achievement that meets the course requirements in every respect.
D - achievement that is worthy of credit even though it fails to meet fully the course requirements.
S - achievement that is satisfactory, which is equivalent to a C- or better (achievement required for an S is at the
  discretion of the instructor but may be no lower than equivalent to a C-)
  F (or N) - Represents failure (or no credit) and signifies that the work was either (1) completed but at a level of
  achievement that is not worthy of credit or (2) was not completed and there was no agreement between the instructor
  and the student that the student would be awarded an I (see also I).


  Incomplete Grades
  The grade of "I" is not a regular University grade and cannot be given without special arrangements under very unusual circumstances. It cannot be given merely to extend the time allowed to complete course requirements. If family or personal emergency requires that your attention be diverted from the course and that more time than usual is needed to complete course work, arrangements should be made with the instructor of the course before the quarter ends and consent obtained for receiving an "Incomplete" or "I" grade. These arrangements should be made as soon as the need for an "I" can be anticipated. A written agreement should be prepared indicating when the course assignment will be completed. Normally an "Incomplete" grade for a course should be removed within one quarter of its receipt.


  University Technology Support Services
  Need help with common campus technology issues? Students can get help with general computer, Internet, and network issues in a variety of ways as described on the Office of Information Technology (OIT) help and support page. *Please note that individual instructors cannot help you with issues or problems with your personal computer, network, or Internet connection.


  Student Conduct Code
  The University seeks an environment that promotes academic achievement and integrity, that is protective of free inquiry, and that serves the educational mission of the University. Similarly, the University seeks a community that is free from violence, threats, and intimidation; that is respectful of the rights, opportunities, and welfare of students, faculty, staff, and guests of the University; and that does not threaten the physical or mental health or safety of members of the University community. As a student at the University you are expected adhere to Board of Regents Policy: Student Conduct Code. To review the Student Conduct Code, please see: http://regents.umn.edu/sites/default/files/policies/Student_Conduct_Code.pdf. Note that the conduct code specifically addresses disruptive classroom conduct, which means "engaging in behavior that substantially or repeatedly interrupts either the instructor's ability to teach or student learning. The classroom extends to any setting where a student is engaged in work toward academic credit or satisfaction of program-based requirements or related activities."


  Scholastic Dishonesty
  Academic dishonesty, including plagiarism, in any portion of the academic work for a course shall be grounds for receiving a grade of F or N for the entire course. You are expected to do your own academic work and cite sources as necessary. Failing to do so is scholastic dishonesty. Scholastic dishonesty means plagiarizing; cheating on assignments or examinations; engaging in unauthorized collaboration on academic work; taking, acquiring, or using test materials without faculty permission; submitting false or incomplete records of academic achievement; acting alone or in cooperation with another to falsify records or to obtain dishonestly grades, honors, awards, or professional endorsement; altering, forging, or misusing a University academic record; or fabricating or falsifying data, research procedures, or data analysis. (Student Conduct Code: http://regents.umn.edu/sites/default/files/policies/Student_Conduct_Code.pdf) If it is determined that a student has cheated, he or she may be given an "F" or an "N" for the course, and may face additional sanctions from the University. For additional information, please see: http://policy.umn.edu/Policies/Education/Education/INSTRUCTORRESP.html. The Office for Student Conduct and Academic Integrity has compiled a useful list of Frequently Asked Questions pertaining to scholastic dishonesty: http://www1.umn.edu/oscai/integrity/student/index.html. If you have additional questions, please clarify with your instructor for the course. Your instructor can respond to your specific questions regarding what would constitute scholastic dishonesty in the context of a particular class-e.g., whether collaboration on assignments is permitted, requirements and methods for citing sources, etc.


  Appropriate Student Use of Class Notes and Course Materials
  Taking notes is a means of recording information but more importantly of personally absorbing and integrating the educational experience. However, broadly disseminating class notes beyond the classroom community or accepting compensation for taking and distributing classroom notes undermines instructor interests in their intellectual work product while not substantially furthering instructor and student interests in effective learning. Such actions violate shared norms and standards of the academic community. For additional information, please see: http://policy.umn.edu/Policies/Education/Education/STUDENTRESP.html.


  Sexual Harassment
  "Sexual harassment" means unwelcome sexual advances, requests for sexual favors, and/or other verbal or physical conduct of a sexual nature. Such conduct has the purpose or effect of unreasonably interfering with an individual's work or academic performance or creating an intimidating, hostile, or offensive working or academic environment in any University activity or program. Such behavior is not acceptable in the University setting. For additional information, please consult Board of Regents Policy: http://regents.umn.edu/sites/default/files/policies/SexHarassment.pdf


  Equity, Diversity, Equal Opportunity, and Affirmative Action
  The University provides equal access to and opportunity in its programs and facilities, without regard to race, color, creed, religion, national origin, gender, age, marital status, disability, public assistance status, veteran status, sexual orientation, gender identity, or gender expression. For more information, please consult Board of Regents Policy: http://regents.umn.edu/sites/default/files/policies/Equity_Diversity_EO_AA.pdf.

  Disability Accommodations
  The University of Minnesota is committed to providing equitable access to learning opportunities for all students. Disability Services (DS) is the campus office that collaborates with students who have disabilities to provide and/or arrange reasonable accommodations. If you have, or think you may have, a disability (e.g., mental health, attentional, learning, chronic health, sensory, or physical), please contact DS at 612-626-1333 to arrange a confidential discussion regarding equitable access and reasonable accommodations. If you are registered with DS and have a current letter requesting reasonable accommodations, please contact your instructor as early in the semester as possible to discuss how the accommodations will be applied in the course. For more information, please see the DS website, https://diversity.umn.edu/disability/.


  Mental Health and Stress Management
  As a student you may experience a range of issues that can cause barriers to learning, such as strained relationships, increased anxiety, alcohol/drug problems, feeling down, difficulty concentrating and/or lack of motivation. These mental health concerns or stressful events may lead to diminished academic performance and may reduce your ability to participate in daily activities. University of Minnesota services are available to assist you. You can learn more about the broad range of confidential mental health services available on campus via the Student Mental Health website: http://www.mentalhealth.umn.edu.


  Academic Freedom and Responsibility: for courses that do not involve students in research
  Academic freedom is a cornerstone of the University. Within the scope and content of the course as defined by the instructor, it includes the freedom to discuss relevant matters in the classroom. Along with this freedom comes responsibility. Students are encouraged to develop the capacity for critical judgment and to engage in a sustained and independent search for truth. Students are free to take reasoned exception to the views offered in any course of study and to reserve judgment about matters of opinion, but they are responsible for learning the content of any course of study for which they are enrolled.* Reports of concerns about academic freedom are taken seriously, and there are individuals and offices available for help. Contact the instructor, the Department Chair, your adviser, the associate dean of the college, or the Vice Provost for Faculty and Academic Affairs in the Office of the Provost.
  * Language adapted from the American Association of University Professors "Joint Statement on Rights and Freedoms of Students"
