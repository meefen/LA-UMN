## Instructor Information

* Instructor: Bodong Chen, Assistant Professor
* Email: bodong.chen@gmail.com (needs an @umn.edu address)
* Phone: xxx-xxx-xxxx
* Office: LTML, 1954 Buford Avenue, St. Paul, MN 55108
* Office Hours: By Appointment

## Course Description

Learning analytics is an nascent field of research that aspires to turn educational data into actionable knowledge, in order to optimize learning and the environments in which it occurs. This course will provide a general, non-technical survey of learning analytics and its application in various educational contexts. In particular, we will discuss theoretical foundations of this field, explore new forms of assessment, get acquainted with popular data mining techniques, review learning analytical tools deployed in various settings, and design/develop new analytic tools by ourselves, with emphasis on assessing emergent competencies of interest in the knowledge age. Additional supports will be provided as well for students interested in pursuing deeper issues in any of these areas. Overall, this will be a great course to get a broad overview of the current state and possible future directions of learning analytics.

## Course Audience

The course is designed for a broad audience. All graduate students interested in learning analytics and its application in specific educational areas (e.g., STEM, literacies, adult education) are welcomed.

Prerequisites: None, but some prior knowledge in learning theories, assessment, and/or data science recommended.

## Course Objectives

By the end of the course, students should:

1. Understand the logic of analytics;
2. Identify and describe key epistemological, pedagogical, ethical, and technical factors related to the design of learning analytics;
3. Be familiar with the basics of finding, cleaning, and using educational data;
4. Understand some of these popular data mining techniques, including predictive models, text analysis, relationship mining, and social networks;
5. Develop beginning skills necessary to plan and design learning analytics.

## Course Design

This is a _Knowledge Building_ course, which means all course participants (including the instructor) are collectively producing ideas and knowledge as a community, to solve authentic learning analytics problems \footnote{Scardamalia, M. and Bereiter, C. (2003). Knowledge building. In Guthrie, J. W., editor, Encyclopedia of education, volume 17, pages 1370–1373. Macmillan Reference, New York, NY, 2 edition.}. Our top-level goal in this course will be to work as a knowledge building team, _living_ and _exploring_ the capacity of learning analytics in supporting growth in learning in different domains. This overarching goal will be interwoven throughout this course. We will advance this goal through analysis of readings, case studies, and innovative design. 

#### Design Contexts

- Knowledge Building Analytics: Knowledge Building (KB) as a distinctive pedagogy has a long-standing interests in analytics. While using available analytic tools designed to support KB, students are encouraged to advance KB analytics to assess sophisticated phenomena, such as the status of dialogues, meta-dialogues, and community cohesion. 
- [Institute for the Scholarship of Assessment, Learning, and Teaching (iSALT)](http://www.mnsu.edu/its/academic/isalt.html), Minnesota State University, Mankato: iSALT is looking for ways to improve scholarly activities related to effective teaching, learning, and assessment practices, especially through learning analytics. We would have guests from iSALT to present their real-world challenges. Interested students could rally to tackle their challenges.
- Students are more than welcomed to bring in their own design contexts.

#### Course Timeline


_The first five weeks_ are designed to provide an introduction to the field of learning analytics, including its roots, basic logic, data mining techniques, and case studies. These weeks feature both _theoretical discussion_, with emphasis on the assumptions underlying analytics tools and projects, and _hands-on learning activities_. During the process, we will continue to articulate our collective design goal and form _working groups (WGs)_ around emergent sub-goals in different design contexts. 

_The second part_ of the course features five "themes" representing key research areas in the field of learning analytics. Each student will sign up for one theme, and students under a same theme form a special interest groups (SIG). Each SIG is expected to take a lead on its theme---presenting key ideas, leading discussion, and making connections with our collective goal. During the time, each WG will keep advancing their designs, in the forms of design specs, mock-ups, or functioning prototypes.

The class will use _the final weeks_ to further advance our designs and create synthesis. Each WG will present their work in front of the class. We will together reflect on our designs and the extent to which our collective goal is achieved.

## Supporting Environments

- Major environment: Knowledge Forum (KF) - http://kf.utoronto.ca:8080/kforum/ 
- Participants can also use media of their choices -- e.g., a blog, Youtube, Twitter -- and we will aggregate produced artifacts through a #LAUMN course hashtag.

## Analytics Tools

- R
- Gephi
- RapidMiner
- ...

## Course Evaluation

#### Parameters

- _Group- and Individual-Assessment_: Students will be assessed both individually and as a group (SIG and WG)
- _Self- and Peer-Assessment_: Students will be assessing both by themselves and peers, based their contributions to the community and personal growth

#### Grading

- Class participation, 30%
- SIG theme presentation, 25%
- WG final presentation, 25%
- Final assignment, 20%

Class participation involves active (and constructive) participation in online and offline discussions. Evaluation will be based on both numeric metrics exported from Knowledge Forum and qualitative assessment of one's contribution to discussions.

Group presentations will be peer-assessed: When one group presents, other groups will evaluate the presentation following a given rubric. Students in a same group get a same score. Aggregated results will be provided to them after the class. 

Final assignment: Students would have the choice among _writing a reflective essay (not exceeding 4,000 words)_, _preparing a portfolio Knowledge Forum note_ reflecting on one's journey in the course, or _producing design documents of a proposed analytic tool_. 

\newpage

## __Class Schedule__

 - Special Topics: Learning Analytics 
 - Professor Bodong Chen

#### __Week 1: Introduction__

#### Readings

- None

#### Learning Activities

- Discussion: (1) Introduce yourself and tell people why you're here! (2) Discuss learning analytics research and projects you are aware of.

#### __Week 2: Learning Analytics: A Brief Overview__

#### Readings

- Siemens, G. (2013). Learning Analytics: The Emergence of a Discipline. American Behavioral Scientist, 0002764213498851. doi:10.1177/0002764213498851
- Shum, S. B. (2012). [UNESCO Policy Brief: Learning Analytics](http://www.iite.unesco.org/publications/3214711/). Technical Report, UNESCO Institute for Information Technologies in Education.
- Optional
    - Long, P. and Siemens, G. (2011). [Penetrating the Fog: Analytics in Learning and Education](http://www.educause.edu/EDUCAUSE+Review/EDUCAUSEReviewMagazineVolume46/PenetratingtheFogAnalyticsinLe/235017). Educause Review, 46(5):30–32. 
	- Siemens, G. (2012). Learning analytics. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK ’12 (p. 4). New York, New York, USA: ACM Press. doi:10.1145/2330601.2330605
	- [Video - Introduction to LAK13, by George Siemens](https://s3.amazonaws.com/LAK13/player.html)
	- Suthers, D., & Rosen, D. (2011). A unified framework for multi-level analysis of distributed learning. Proceedings of the 1st International Conference on Learning Analytics and Knowledge - LAK ’11 (pp. 64–74). New York, New York, USA: ACM Press. doi:10.1145/2090116.2090124
	- Olmos, M. and Corrin, L. (2012). Academic analytics in a medical curriculum: Enabling educational excellence. Australasian Journal of Educational Technology, 28(1):1–15.

#### Activities

- Discussion: Discuss readings in KF
- Begin planning your analytics project

#### __Week 3: What to Assess: New Competencies in the Knowledge Age__

#### Readings

- Soland, J., Hamilton, L. S., and Stecher, B. M. (2013). [Measuring 21st century competencies: Guidance for educators](http://asiasociety.org/files/gcen-measuring21cskills.pdf). Report, Asia Society Global Cities Education network.
- Shum, S. B. and Crick, R. D. (2012). Learning dispositions and transferable competencies. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK ’12, page 92, New York, New York, USA. ACM Press.
- Dawson, S. and Siemens, G. (2014). Analytics to literacies: The development of a learning analytics framework for multiliteracies assessment. International Review of Research in Open and Distance Learning, 15(4):284–305.
- Optional
	- Binkley, M., Erstad, O., Herman, J., Raizen, S., Riple, M., Miller-Ricci, M., and Rumble, M. (2012). Defining 21st century skills. In Griffin, P., McGaw, B., and Care, E., editors, Assessment and Teaching of 21st Century Skills, chapter 2, pages 17–66. Springer.

#### Activities

- Virtual meeting with our guest speaker: Simon Buckingham Shum / Marlene Scardamalia
- Discuss readings in KF

#### __Week 4: Explore Hidden Assumptions: Epistemology, Pedagogy, Assessment and Learning Analytics__

#### Readings

- Knight, S., Buckingham Shum, S., and Littleton, K. (2013). Epistemology, pedagogy, assessment and learning analytics. In Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK ’13, page 75, New York, New York, USA. ACM Press.
- Stahl, G. (2013). Learning across Levels. International Journal of Computer-Supported Collaborative Learning, 8(1):1–12.

#### Activities

- Meet our guest speaker: Simon Knight, Open University
- KF discussion
  - Discuss readings
  - Explore design challenges and identify collective design goals


#### __Week 5: Educational Data Mining: A Survey of Important Algorithms, Techniques, and Their Applications__

#### Readings

- Baker, R.S.J.d., Yacef, K. (2009) The State of Educational Data Mining in 2009: A Review and Future Visions. Journal of Educational Data Mining, 1 (1), 3-17.
- Scheuer, O., Mclaren, B.M. (2011) Educational Data Mining. In N. Seel (Ed.) Encyclopedia of the Sciences of Learning.
- Optional
	- Romero, C., Ventura, S. (2007) A Survey from 1995 to 2005. Expert Systems with Applications, 33 (1), 135-146.
	- [The Fourth Paradigm: Data-Intensive Scientific Discovery](http://research.microsoft.com/en-us/collaboration/fourthparadigm/)
	- Resources from [Ryan Baker's "MOOT"](http://www.columbia.edu/~rsb2162/bigdataeducation.html)

#### Activities

- __SIG signup__; 2-3 students per group
- KF discussion
  - Discuss readings
  - Further articulate design goals

#### __Week 6: Cases and Examples of Learning Analytics__

#### Readings and Resources:

- Arnold, K.E. (2010). Signals: Applying academic analytics. Educause Quarterly, 33, 1-10.
- [Social Networks Adapting Pedagogical Practice (SNAPP)](http://www.snappvis.org/)
- [Improving Retention by Identifying and Supporting "At-Risk" Students](http://www.educause.edu/ero/article/improving-retention-identifying-and-supporting-risk-students)
- Teplovs, C., Donoahue, Z., Scardamalia, M., and Philip, D. (2007). Tools for Concurrent, Embedded, and Transformative Assessment of Knowledge Building Processes and Progress. In Proceedings of the 8th iternational conference on Computer supported collaborative learning, pages 721– 723, New Brunswick, New Jersey, USA. International Society of the Learning Sciences.

#### Activities

- Meet our guest speaker from iSALT, Vicky Cai, who would present their design context
- Discuss readings in KF
- Explore analytic tools provided in Knowledge Forum
- Finalize design goals; __WG signup__

#### __Week 7: Learning and Knowledge Growth (theme 1)__

#### Readings

- Schwarz, C. V., Reiser, B. J., Davis, E. A., Kenyon, L., Ach ́er, A., Fortus, D., Shwartz, Y., Hug, B., and Krajcik, J. (2009). Developing a learning progression for scientific modeling: Making scientific modeling accessible and meaningful for learners. Journal of Research in Science Teaching, 46(6):632–654.
- Bull, S. and Kay, J. (2010). Open learner models. In Nkambou, R., Bordeau, J., and Miziguchi, R., editors, Advances in Intelligent Tutoring Systems, chapter 15, pages 318–338. Springer.
- Desmarais, M. C., & Baker, R. S. J. d. (2011). A review of recent advances in learner and skill modeling in intelligent learning environments. User Modeling and User-Adapted Interaction, 22(1-2), 9–38. doi:10.1007/s11257-011-9106-8

#### Activities

- Designed by the SIG (with assistance from the instructor)

#### __Week 8: Social Networks (theme 2)__

#### Readings

- Haythornthwaite, C. (1996). Social network analysis: An approach and technique for the study of information exchange. Library & Information Science Research, 18(4):323–342.
- Grunspan, D. Z., Wiggins, B. L., & Goodreau, S. M. (2014). Understanding Classrooms through Social Network Analysis: A Primer for Social Network Analysis in Education Research. CBE-Life Sciences Education, 13(2), 167–178. doi:10.1187/cbe.13-08-0162
- Oshima, J., Oshima, R., and Matsuzawa, Y. (2012). Knowledge Building Discourse Explorer: a social network analysis application for knowledge building discourse. Educational Technology Research and Development, 60(5):903–921.
- Chen, B., Chen, X, & Xing, W. (submitted). Twitter Archeology of Learning Analytics and Knowledge Conferences. Paper submitted to the 2015 Learning Analytics and Knowledge Conference.

#### Activities

- Designed by the SIG (with assistance from the instructor)
- Gephi

#### __Week 9: Natural Language Processing (theme 3)__

#### Readings

- Rohrer, R., Ebert, D., and Sibert, J. (1998). The shape of Shakespeare: visualizing text using implicit surfaces. In Proceedings of IEEE Symposium on Information Visualization, pages 121–129. IEEE Comput. Soc.
- Rose, C. P., Wang, Y.-C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., and Fis- cher, F. (2008). Analyzing collaborative learning processes automatically: Exploiting the advances of com- putational linguistics in computer-supported collaborative learning. International Journal of Computer- Supported Collaborative Learning, 3(3):237–271.
- Optional
    - Shermis, M. D. (2014). State-of-the-art automated essay scoring: Competition, results, and future directions from a United States demonstration. Assessing Writing, 20, 53–76.
	- Rose, C. P. and Tovares, A. (2014). What Sociolinguistics and Machine Learning Have to Say to One Another about Interaction Analysis. In Resnick, L., Asterhan C., and Clarke S., editors, Socializing Intelligence Through Academic Talk and Dialogue. American Educational Research Association, Washington, D.C.
	- Simsek, D., Buckingham Shum, S., Sandor, A., De Liddo, A., and Ferguson, R. (2013). Xip dashboard: visual analytics from automated rhetorical parsing of scientific metadiscourse. In 1st International Workshop on Discourse-Centric Learning Analytics.

#### Activities

- Designed by the SIG (with assistance from the instructor)

#### __Week 10: Discourse and Metadiscourse (theme 4)__

#### Readings

- Zhang, J., Lee, J., and Wilde, J. (2012). Metadiscourse to foster student collective responsibility for deepening inquiry. In van Aalst, J., Thompson, K., Jacobson, M. J., and Reimann, P., editors, The future of learning: Proceedings of the 10th international conference of the learning sciences (ICLS 2012) - Volume 1, Full Papers, pages 395–402. ISLS, Sydney, Australia.
- Resendes, M., Chen, B., Acosta, A., and Scardamalia, M. (2013). The Effect of Formative Feedback on Vocabulary Use and Distribution of Vocabulary Knowledge in a Grade Two Knowl- edge Building Class. In Rummel, N., Kapur, M., Nathan, M., and Puntambekar, S., editors, To See the World and a Grain of Sand: Learning across Levels of Space, Time, and Scale: CSCL 2013 Conference Proceedings Volume 1 - Full Papers & Symposia, pages 391–398. International Society of the Learning Sciences.
- Optional
	- Chen, B. and Resendes, M. (2014). Uncovering what matters: Analyzing transitional relations among contribution types in knowledge-building discourse. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge - LAK ’14, pages 226–230, New York, New York, USA. ACM Press.
	- Graesser, A. C. and McNamara, D. S. (2011). Computational analyses of multilevel discourse comprehension. Topics in Cognitive Science, 3(2):371–398.


<!-- ### Personalization and Adaptation (theme 4) -->

#### __Week 11: Prediction and Intervention (theme 5)__

#### Readings

- Pardos, Z.A., Baker, R.S.J.d., San Pedro, M.O.C.Z., Gowda, S.M., Gowda, S.M. (2013). Affective states and state tests: Investigating how affect throughout the school year predicts end of year learning outcomes. In Proceedings of the 3rd International Conference on Learning Analytics and Knowledge.
- DeBoer, J. and Breslow, L. (2014). Tracking progress: Predictors of students’ weekly achievement during a circuits and electronics mooc. In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14, pages 169–170, New York, NY, USA. ACM.
- Kloft, M., Stiehler, F., Zheng, Z., and Pinkwart, N. (2014). Predicting mooc dropout over weeks using machine learning methods. In Modeling Large Scale Social Interaction in Massively Open Online Courses Workshop (EMNLP 2014).
- Baker, R. S. J. d., D’Mello, S. K., Rodrigo, M. M. T., & Graesser, A. C. (2010). Better to be frustrated than bored: The incidence, persistence, and impact of learners’ cognitive–affective states during interactions with three different computer-based learning environments. International Journal of Human-Computer Studies, 68(4), 223–241. doi:10.1016/j.ijhcs.2009.12.003

<!-- ### Classroom Interventions with Learning Analytics -->

#### __Week 12: Preparing for Final Presentations__

WGs work to prepare for the final presentations.

#### __Week 13 and 14: Final Presentations and Reflection__

WGs present their design or implementation of learning analytics to address an authentic problem.

#### Reflection

- Ethics and privacy issues
- Classroom interventions with learning analytics

\newpage

## Additional Bibliography

- Baker, R. S. J. d., Gowda, S. M., and Corbett, A. (2011). Automatically detecting a student’s preparation for future learning: Help use is key. In Proceedings of the 4th International Conference on Educational Data Mining, pages 179–188.
- Baker, R. S., Hershkovitz, A., Rossi, L. M., Goldstein, A. B., and Gowda, S. M. (2013). Predicting Robust Learning With the Visual Form of the Moment-by-Moment Learning Curve. Journal of the Learning Sciences, 22(4):639–666.
- Bienkowski, M., Feng, M., and Means, B. (2012). Enhancing Teaching and Learning Through Educational Data Mining and Learning Analytics: An issue brief.
- Blikstein, P. (2011). Using learning analytics to assess students’ behavior in open-ended programming tasks. In Proceedings of the 1st International Conference on Learning Analytics and Knowl- edge - LAK ’11, page 110, New York, New York, USA. ACM Press.
- Burstein, J., Marcu, D., Andreyev, S., and Chodorow, M. (2001). Towards automatic classification of discourse elements in essays. In Proceedings of the 39th annual Meeting on Association for Computational Linguistics, pages 98–105. Association for Computational Linguistics.
- Calvo, R. A. and D’Mello, S. (2010). Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affective Computing, 1(1):18– 37.
- Chiu, M. M. (2008). Flowing Toward Correct Contributions During Group Problem Solving: A Statistical Discourse Analysis. Journal of the Learning Sciences, 17(3):415–463.
- Coffrin, C., Corrin, L., de Barba, P., and Kennedy, G. (2014). Visualizing patterns of student engagement and performance in MOOCs. In Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK ’14, pages 83–92, New York, New York, USA. ACM Press.
- D’Mello, S., Olney, A., and Person, N. (2010). Mining Collaborative Patterns in Tutorial Dialogues. Journal of Educational Data Mining, 2(1):1–37.
- Dyke, G., Kumar, R., Ai, H., and Ros ́e, C. P. (2012). Challenging assumptions: Using sliding window visualizations to reveal time-based irregularities in CSCL processes. In van Aalst, J., Thompson, K., Jacobson, M. J., and Reimann, P., editors, The future of learning: Proceedings of the 10th international conference of the learning sciences (ICLS 2012) - Volume 1, Full Papers, pages 363–370. ISLS, Sydney, Australia.
- Ferguson, R. (2012) Learning analytics: drivers, developments and challenges. International Journal of Technology Enhanced Learning (IJTEL), 4 (5/6), 304-317.
- Gobert, J. D., Sao Pedro, M., Raziuddin, J., and Baker, R. S. (2013). From Log Files to Assessment Metrics: Measuring Students’ Science Inquiry Skills Using Educational Data Mining. Journal of the Learning Sciences, 22(4):521–563.
- Halatchliyski, I., Hecking, T., G ̈ohnert, T., and Hoppe, H. U. (2013). Analyzing the flow of ideas and profiles of contributors in an open learning community. In Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK ’13, page 66, New York, USA. ACM Press.
- Halevy, A., Norvig, P., and Pereira, F. (2009). The unreasonable effectiveness of data. Intelligent Systems, IEEE, 24(2):8–12.
- Haythornthwaite, C., de Laat, M., and Dawson, S. (2013). Introduction to the Special Issue on Learning Analytics. American Behavioral Scientist, 57(10):1371–1379.
- Holloway, T., Bozicevic, M., and B ̈orner, K. (2007). Analyzing and visualizing the semantic coverage of Wikipedia and its authors. Complexity, 12(3):30–40.
- Howley, I., Mayfield, E., and Ros ́e, C. P. (2013). Linguistic analysis methods for studying small groups.
- Lee, V. R., & Drake, J. (2013). Quantified Recess: Design of an Activity for Elementary Students Involving Analyses of Their Own Movement Data. In Proceedings of the 12th International Conference on Interaction Design and Children (pp. 273–276). New York, NY, USA: ACM. doi:10.1145/2485760.2485822
- Martin, T. and Sherin, B. (2013). Learning Analytics and Computational Tech- niques for Detecting and Evaluating Patterns in Learning: An Introduction to the Special Issue. Journal of the Learning Sciences, 22(4):511–520.
- Mattingly, K., Rice, M., and Berge, Z. (2012). Learning analytics as a tool for closing the assessment loop in higher education. Knowledge Management & E-Learning: An International Journal, 4(3):236–247.
- New York Times. (2014, August 6). [Is Big Data Spreading Inequality?](http://www.nytimes.com/roomfordebate/2014/08/06/is-big-data-spreading-inequality) Retrieved August 20, 2014
- Romero, C., Ventura, S., Espejo, P. G., and Herv ́as, C. (2008). Data Mining Algo- rithms to Classify Students. Network, pages 20–21.
- Siemens, G. (2013) Learning Analytics: The Emergence of a Discipline. American Behavioral Scientist, 57 (10), 1380-1400.
- Siemens, G., Baker, R.S.J.d. (2012) Learning Analytics and Educational Data Mining: Towards Communication and Collaboration. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge.
- Suthers, D. and Rosen, D. (2011). A unified framework for multi-level analysis of distributed learning. In Proceedings of the 1st International Conference on Learning Analytics and Knowledge - LAK ’11, pages 64–74, New York, New York, USA. ACM Press.
- Wise, A. F. (2014). Designing pedagogical interventions to support student use of learning analytics. In Proceedins of the Fourth International Conference on Learning Analytics And Knowledge - LAK ’14, pages 203–211, New York, New York, USA. ACM Press.


## University Policies

TO BE ADDED
